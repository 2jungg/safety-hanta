# Safety-Hanta 인수인계
## 1. 개요
본 프로젝트는 kubernetes(k8s) 환경에서 실시간으로 VLM이 영상을 분석하여 사고를 판별하는 프로젝트입니다.
docker에 대한 기본적인 사용법과 k8s에 대한 기본적인 사용법을 숙지하고 있어야 합니다.
현재 kind를 사용하여 클러스터를 구성하고 있습니다. run.sh를 실행하면 모든 과정이 자동으로 진행됩니다.
```bash
. ./run.sh
```
단, DP CCTV와 연결하기 위해서는 connect.sh를 실행하여 ssh 터널링을 이용해야 합니다.
이 때 connect.sh를 백그라운드로 실행하면 편리하기에, ubuntu의 screen을 사용하면 편리합니다.
```bash
# screen 실행
screen
# connect.sh 실행
. ./connect.sh
# screen 종료 (백그라운드 실행)
ctrl+a, d
# screen 재접속
screen -r
```

## 2. 아키텍쳐
본 프로젝트는 크게 4개의 파이프라인으로 구성되어 있습니다.
1. (선택사항) rtsp-simulator: local video를 rtsp를 사용하여 영상 스트리밍을 구현합니다.
2. video-capture: rtsp 스트림에서 영상을 캡처하여 redis queue에 저장합니다.
3. inference: redis queue에서 영상을 가져와 VLM으로 추론합니다.
4. logic & notification: 추론 결과를 바탕으로 로직을 수행하고 알림을 보냅니다.

### 1. rtsp-simulator
k8s/02-rtsp-simulator.yaml을 통해 실행됩니다.
- 기능을 켜고 끌 때에는 replicas를 0(끔), 1(켬)으로 설정하면 됩니다.
- 기본적으로 videos/ 폴더에 있는 accident_video-##.mp4를 rtsp 스트리밍하며, NUM_STREAMS를 통해 스트리밍할 영상의 개수를 조절할 수 있습니다.
- *참고로 k8s에서 폴더들은 전부 volume mount를 통해 연결되어 있기 때문에 참고하시길 바랍니다.

### 2. video-capture
k8s/03-video-capture.yaml을 통해 실행되며, 파이썬 코드는 src/capture/main.py입니다.
- replicas를 통해 실행할 worker의 개수를 조절할 수 있습니다. (3개 실행시 각각 cam0, cam1, cam2를 담당)
- RTSP_BASE_URL을 통해 rtsp 스트림의 주소를 설정할 수 있습니다. (DP CCTV와 rtsp-simulator 모두 사용 가능)
- BUFFER_DURATION을 통해 캡처할 영상의 버퍼 시간을 설정할 수 있습니다. (기본값: 10초)
- 캡처된 영상은 기본적으로 videos/temp_video에 저장됩니다.

### 3. inference
k8s/04-inference.yaml을 통해 실행되며, 파이썬 코드는 src/inference/main.py입니다.
- command에서 src/inference/ 폴더 내의 어떤 파이썬 코드를 실행할 지 설정할 수 있습니다. 가장 최근까지 테스트한 코드는 main_qwen3_reasoning.py입니다. (main.py는 cosmos-reason1 모델을 실행하기 위해 만들었던 코드입니다. 해당 모델의 구조는 qwen2.5 기반입니다)
- MODEL_PATH를 통해 VLM 모델을 설정할 수 있습니다. (기본값: qwen/qwen3-vl-8b-instruct)
- 기본적으로 영상을 기반으로 추론하도록 설정되어 있습니다(영상의 맥락을 고려할 수 있도록). Cosmos-Reason1&2 모델의 경우 4FPS로 학습했다고 하여 추론시 입력도 4FPS로 설정되어 있습니다. (configs/vision_config.yaml 참고) 

추론 속도가 느리며, 정확도가 생각보다 낮기 때문에 다음의 기법을 고려해 볼 수 있습니다.
1. prefix-caching
    - 적용해보았을 때 큰 차이가 없었던 것 같은데, 제대로 벤치마킹을 구성하여 테스트해보면 좋을 것 같습니다.
2. speculative decoding
    - 테스트 해보았을 때 모델 두개를 올리는게 버거워서 고려대상에서 제외되었으나, 더 심도있는 테스트가 진행되면 좋을 것 같습니다
3. 모델 양자화
    - 양자화된 모델을 사용하는 방법도 좋은 방안입니다.
    - dgx-spark의 GB10(블랙웰 아키텍처 기반)는 특히 NVFP4 타입 연산에 최적화되어 있다고 합니다.
4. 추론 엔진
    - 현재 코드는 vllm 추론 엔진을 기반으로 테스트를 진행하였습니다.
    - 허나 최근에 NVIDIA의 TensorRT-LLM이 VLM 추론을 지원하기 시작했습니다.
    - TensorRT-LLM은 vllm보다 더 빠른 추론 속도를 제공한다는 의견이 있어, 한번 확인해보시면 좋을 것 같습니다.
5. few-shot
    - 현재는 text prompt만 사용하고 있습니다.
    - main_qwen3_fewshot.py에서 video prompt와 text prompt 기반 few-shot 추론을 진행해 보았으나, 속도가 너무 늦어져 고려하지 않았습니다.
    - 혹시 더 좋은 방법이 있으면 테스트해보시면 될 것 같습니다.
6. 더 큰 모델 사용
    - 제가 테스트 할 때에는 추론 속도가 주요 고려 요소였는데, 더 정확한 결과가 나오는 것이 마지막즈음에 주요 고려 요소로 바뀌었습니다. 때문에 제가 고려하지 못한 더 큰 모델들을 테스트해보시면 좋을 것 같습니다.
    - Qwen 시리즈 말고도 많이 있으니... 파이팅입니다!
7. fine-tuning
    - 현재는 pre-trained 모델을 그대로 사용하고 있지만, fine-tuning을 통해 정확도를 높일 수 있습니다.
    - 그러나 fine-tuning을 하는 것은 자원의 한계와 video 데이터 수집의 어려움 때문에 고려하지 않았습니다.

### 4. logic & notification
- 추론 결과를 바탕으로 로직을 수행하고 알림을 보냅니다.
- 알림은 기본적으로 Telegram을 통해 전송됩니다.
- Telegram Bot과 채팅방을 설정하는 방법은 AI가 잘 알려줄 것이며, .env 파일을 수정하여 토큰과 챗 아이디를 입력하시면 됩니다.

## 3. 로그 분석
k9s를 통해 클러스터 내의 파드들을 확인할 수 있습니다.
```bash
k9s
```
간단한 단축키와 명령어를 설명드리겠습니다.
- h,j,k,l: vim과 동일하게 상하좌우 이동
- pod를 선택하고 l: pod의 로그를 실시간으로 확인
- pod를 선택하고 e: pod 내부로 쉘 접속
- pod를 선택하고 d: pod의 describe 정보 확인
- pod를 선택하고 ctrl+d: pod 삭제

위 기능들은 kubectl 명령어로도 동일하게 수행할 수 있으며, k9s는 TUI를 통해 로그를 확인할 수 있기에 k9s 사용을 권장드립니다.