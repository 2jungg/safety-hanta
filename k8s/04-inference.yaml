apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference
  labels:
    app: inference
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: inference
  template:
    metadata:
      labels:
        app: inference
    spec:
      nodeSelector:
        role: inference

      containers:
      - name: inference
        image: cosmos-reason1-server:latest
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 1
        command: ["python3", "-u", "src/inference/main_qwen3.py"]
        env:
        - name: MODEL_PATH
          value: "/models/Qwen3-VL-4B-Instruct-NVFP4"
        - name: GPU_MEMORY_UTILIZATION
          value: "0.8"
        - name: LD_LIBRARY_PATH
          value: "/usr/lib/aarch64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
        - name: REDIS_HOST
          value: "redis-service"
        - name: REDIS_PORT
          value: "6379"
        - name: MIN_BATCH_SIZE
          value: "3"
        - name: BATCH_TIMEOUT
          value: "1.0"
        volumeMounts:
        - name: model-volume
          mountPath: /models
        - name: videos-volume
          mountPath: /videos
        - name: src-volume
          mountPath: /app/src/inference
      volumes:
      - name: videos-volume
        hostPath:
          path: /videos
          type: Directory
      - name: src-volume
        hostPath:
          path: /src_host/inference
          type: Directory
      - name: model-volume
        hostPath:
          path: /models
          type: Directory
